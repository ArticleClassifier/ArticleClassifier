### ICA 2022 submission source code - BERT and active learning experiment

This repository includes the source code of ICA 2022 submission "A workflow for building article classifier: Using BERT and active learning in article classification". The source code introduces an experiment comparing active learning approach and random-sampling approach for fine-tuning Finnish BERT model FinBERT.  

Technical implementation of fine-tuning BERT is done by following [the tutorial by Abdou Rockikz](https://www.thepythoncode.com/article/finetuning-bert-using-huggingface-transformers-python). The experiment code was executed in Google Colab GPU environment. 

Unfortunately, the dataset used in the code can not be published due to copyright reasons. 

## References 

FinBERT

Virtanen, A., Kanerva, J., Ilo, R., Luoma, J., Luotolahti, J., Salakoski, T., Ginter, F., & Pyysalo, S. (2019). Multilingual is not enough: BERT for Finnish. ArXiv, abs/1912.07076.

Description of the dataset used in the experiment 

Toivanen, P., Nelimarkka, M., & Valaskivi, K. (2021). Remediation in the hybrid media environment: Understanding countermedia in context. New Media & Society. https://doi.org/10.1177/1461444821992701




